import requests
from bs4 import BeautifulSoup
from playwright.async_api import async_playwright, Error
from playwright.async_api import TimeoutError as PlaywrightTimeoutError
from threading import Thread
import asyncio
import cloudscraper
import re
from urllib.parse import urljoin
import time
def resolve_bing_redirect(url: str) -> str:
    """
    Nh·∫≠n v√†o 1 URL (th∆∞·ªùng l√† c·ªßa Bing redirect), 
    tr·∫£ v·ªÅ URL th·∫≠t n·∫øu ph√°t hi·ªán ƒë∆∞·ª£c, 
    c√≤n n·∫øu kh√¥ng th√¨ tr·∫£ v·ªÅ ch√≠nh URL g·ªëc.
    """
    scraper = cloudscraper.create_scraper(
        browser={'browser': 'chrome', 'platform': 'windows', 'mobile': False}
    )
    
    try:
        response = scraper.get(url, timeout=10)
        html = response.text
        
        # T√¨m ƒë∆∞·ªùng d·∫´n th·ª±c trong JavaScript: var u = "https://..."
        match = re.search(r'var\s+u\s*=\s*"([^"]+)"', html)
        if match:
            real_url = match.group(1)
            print(f"üîó ƒê√£ ph√°t hi·ªán redirect Bing ‚Üí {real_url}")
            return real_url
        
        # N·∫øu kh√¥ng c√≥, th·ª≠ xem c√≥ th·∫ª meta refresh
        match_meta = re.search(r'<meta[^>]*url=([^">]+)', html, re.IGNORECASE)
        if match_meta:
            real_url = match_meta.group(1)
            print(f"üîó ƒê√£ ph√°t hi·ªán redirect meta ‚Üí {real_url}")
            return real_url
        
        print("‚ö†Ô∏è Kh√¥ng ph√°t hi·ªán redirect, d√πng URL g·ªëc.")
        return url

    except Exception as e:
        print(f"‚ùå L·ªói khi ki·ªÉm tra Bing redirect: {e}")
        return url
    
def bing_search(content, page=3):
    response = requests.get("https://bing.com")
    # print(f"Bing status: {response.status_code}")
    headers = response.headers
    data_response = []
    headers = {
    "Cookie": headers['Set-Cookie']
}

    for i in range(page):
        start = i * 10 + 1
        url = f'https://www.bing.com/search?q="{content}"&first={start}&cc=VN&setlang=vi'
        
        # G·ª≠i request v·ªõi cloudscraper
        response = requests.get(url,headers=headers)
        raw_text = response.text

        soup = BeautifulSoup(raw_text, 'html.parser')
        ol_element = soup.find("ol", id="b_results")
        if not ol_element:
            continue

        li_elements = ol_element.find_all("li", class_=["b_algo", "b_algo b_vtl_deeplinks qbrs"])
        
        for li in li_elements:
            h2 = li.find("h2")
            if not h2 or not h2.find("a"):
                continue
            title = h2.get_text(strip=True)
            href = h2.find("a")["href"]

            desc_tag = li.find("p", class_="b_lineclamp2")
            description = desc_tag.get_text(strip=True) if desc_tag else ""
            data_ = {
                "title": title,
                "description": description,
                "link": resolve_bing_redirect(href)
            }
            print(data_)
            data_response.append(data_)
    # print(data_response)
    return data_response

async def crawl_page(url: str, timeout_each=5000):
    """Crawl 1 trang web, gi·ªõi h·∫°n ri√™ng cho page.goto."""
    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=True)
        page = await browser.new_page()
        try:
            # 1Ô∏è‚É£ ƒêi ƒë·∫øn trang, ch·ªù DOM s·∫µn s√†ng
            response = await page.goto(url, timeout=timeout_each, wait_until="load")

            status = response.status if response else None
            print(f"üì° HTTP status: {status}")

            # 2Ô∏è‚É£ N·∫øu th√†nh c√¥ng -> l·∫•y HTML
            if status in [200, 201,204]:
                html = await page.content()
                html = BeautifulSoup(html,'html.parser')
                imgs_attr = []
                imgs = html.find_all("img")
                
                for img in imgs:
                    src = img.get("src")
                    alt = img.get("alt")

                    # ‚úÖ Chuy·ªÉn link t∆∞∆°ng ƒë·ªëi th√†nh tuy·ªát ƒë·ªëi
                    if src:
                        src = urljoin(url, src)

                    imgs_attr.append({"src": src, "alt": alt})
                raw_text = html.text
                html = re.sub(r"\n", "", raw_text)
                return html, imgs_attr
            else:
                return "", []

        except PlaywrightTimeoutError:
            print("‚è∞ Timeout n·ªôi b·ªô (page.goto) ‚Äî th·ª≠ l·∫•y HTML hi·ªán c√≥...")

            try:
                # ‚è≥ ƒê·ª£i 0.5s ƒë·ªÉ trang ·ªïn ƒë·ªãnh tr∆∞·ªõc khi l·∫•y content
                await asyncio.sleep(0.5)
                html = await page.content()
            except Error:
                print("‚ö†Ô∏è Trang ƒëang chuy·ªÉn h∆∞·ªõng, ƒë·ª£i th√™m r·ªìi th·ª≠ l·∫°i l·∫ßn 2...")
                await asyncio.sleep(1)
                try:
                    html = await page.content()
                except Error:
                    print("‚ùå Kh√¥ng th·ªÉ l·∫•y HTML, b·ªè qua trang n√†y.")
                    return "", []

            # ‚úÖ Ti·∫øp t·ª•c x·ª≠ l√Ω HTML n·∫øu l·∫•y ƒë∆∞·ª£c
            html = BeautifulSoup(html, 'html.parser')
            imgs_attr = []
            imgs = html.find_all("img")
            for img in imgs:
                src = img.get("src")
                alt = img.get("alt")
                if src:
                    src = urljoin(url, src)
                imgs_attr.append({"src": src, "alt": alt})
            raw_text = html.text
            html = re.sub(r"\n", "", raw_text)
            return html, imgs_attr
        except:
            print(f"Crawl th·∫•t b·∫°i")
            return "", []
        finally:
            # ƒê·∫£m b·∫£o browser lu√¥n ƒë∆∞·ª£c ƒë√≥ng, k·ªÉ c·∫£ khi timeout t·ªïng th·ªÉ
            await browser.close()


async def crawl_with_total_timeout(url, total_timeout=10):
    print(f"ƒêang crawl {url}")
    start_time = time.perf_counter()  # ‚è± B·∫Øt ƒë·∫ßu ƒë·∫øm
    """Gi·ªõi h·∫°n t·ªïng th·ªùi gian crawl to√†n b·ªô."""
    try:
        # D√πng asyncio.wait_for ƒë·ªÉ gi·ªõi h·∫°n t·ªïng th·ªùi gian
        html, info = await asyncio.wait_for(crawl_page(url), timeout=total_timeout)
        elapsed = time.perf_counter() - start_time  # ‚è± Th·ªùi gian ƒë√£ tr√¥i qua
        print(f"‚úÖ Ho√†n t·∫•t trong {elapsed:.2f} gi√¢y.")
        return html, info

    except asyncio.TimeoutError:
        print("‚è∞ Timeout t·ªïng th·ªÉ! Qu√° 6 gi√¢y r·ªìi.")
        elapsed = time.perf_counter() - start_time  # ‚è± Th·ªùi gian ƒë√£ tr√¥i qua
        print(f"‚úÖ Ho√†n t·∫•t trong {elapsed:.2f} gi√¢y .")
        return "", []

# html, imgs = asyncio.run(crawl_with_total_timeout("https://caodang.fpt.edu.vn/"))
# print(html)
# print(imgs)

async def crawl_list_urls(urls):
    results = []
    for u in urls:
        html, imgs = await crawl_with_total_timeout(u["url"], total_timeout=10)
        results.append({"html": html, "imgs_attr": imgs})
    return results

